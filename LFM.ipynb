{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from six import next\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "\n",
    "with open('reviews_Musical_Instruments_5.json', 'r') as f:\n",
    "    raw_json = f.readlines()\n",
    "    for record in raw_json:\n",
    "        reviews.append(eval(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9751</th>\n",
       "      <td>A1DVUFG2QSJ6IK</td>\n",
       "      <td>B007XH9432</td>\n",
       "      <td>5</td>\n",
       "      <td>1381104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4692</th>\n",
       "      <td>A1365RYO0BLEMI</td>\n",
       "      <td>B000EELFI8</td>\n",
       "      <td>5</td>\n",
       "      <td>1348012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>A1C92SAQFUBJSZ</td>\n",
       "      <td>B0002GXZK4</td>\n",
       "      <td>4</td>\n",
       "      <td>1348876800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6878</th>\n",
       "      <td>A1GMWTGXW682GB</td>\n",
       "      <td>B001PGXKC8</td>\n",
       "      <td>4</td>\n",
       "      <td>1342137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>A24AQ24CD6865K</td>\n",
       "      <td>B000EEJJ5Y</td>\n",
       "      <td>5</td>\n",
       "      <td>1399334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6793</th>\n",
       "      <td>A3JXFUXN03IUT5</td>\n",
       "      <td>B001PGXHX0</td>\n",
       "      <td>5</td>\n",
       "      <td>1395446400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>A2UYE434RFINE</td>\n",
       "      <td>B0002GLDQM</td>\n",
       "      <td>3</td>\n",
       "      <td>1360195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>AA1ZYCEKJGG3A</td>\n",
       "      <td>B000ZJTPLG</td>\n",
       "      <td>2</td>\n",
       "      <td>1354320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10102</th>\n",
       "      <td>A2U1Z3TZ4P76JB</td>\n",
       "      <td>B00BTGMI5O</td>\n",
       "      <td>4</td>\n",
       "      <td>1393977600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9038</th>\n",
       "      <td>AU2TEZYYXKLL</td>\n",
       "      <td>B005A09I7Q</td>\n",
       "      <td>3</td>\n",
       "      <td>1374451200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerID        asin  overall  unixReviewTime\n",
       "9751   A1DVUFG2QSJ6IK  B007XH9432        5      1381104000\n",
       "4692   A1365RYO0BLEMI  B000EELFI8        5      1348012800\n",
       "2624   A1C92SAQFUBJSZ  B0002GXZK4        4      1348876800\n",
       "6878   A1GMWTGXW682GB  B001PGXKC8        4      1342137600\n",
       "4595   A24AQ24CD6865K  B000EEJJ5Y        5      1399334400\n",
       "6793   A3JXFUXN03IUT5  B001PGXHX0        5      1395446400\n",
       "2336    A2UYE434RFINE  B0002GLDQM        3      1360195200\n",
       "5981    AA1ZYCEKJGG3A  B000ZJTPLG        2      1354320000\n",
       "10102  A2U1Z3TZ4P76JB  B00BTGMI5O        4      1393977600\n",
       "9038     AU2TEZYYXKLL  B005A09I7Q        3      1374451200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_info = json.dumps(reviews)\n",
    "\n",
    "df = pd.read_json(json_info)\n",
    "df = df[['reviewerID', 'asin', 'overall', 'unixReviewTime']].copy()\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "u_num = len(df['reviewerID'].unique())\n",
    "i_num = len(df['asin'].unique())\n",
    "\n",
    "batch_size = 100 \n",
    "dims = 5          \n",
    "max_epochs = 50   \n",
    "\n",
    "place_device = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    rows = len(df)\n",
    "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    split_index = int(rows * 0.9)\n",
    "\n",
    "    df_train = df[0:split_index]\n",
    "    df_test = df[split_index:].reset_index(drop=True)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(user_batch, item_batch, user_num, item_num, dim=5, device=\"/cpu:0\"):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        with tf.variable_scope('lsi',reuse=tf.AUTO_REUSE):\n",
    "            bias_global = tf.get_variable(\"bias_global\", shape=[])\n",
    "\n",
    "            w_bias_user = tf.get_variable(\"embd_bias_user\", shape=[user_num])\n",
    "            w_bias_item = tf.get_variable(\"embd_bias_item\", shape=[item_num])\n",
    "\n",
    "            bias_user = tf.nn.embedding_lookup(w_bias_user, user_batch, name=\"bias_user\")\n",
    "            bias_item = tf.nn.embedding_lookup(w_bias_item, item_batch, name=\"bias_item\")\n",
    "\n",
    "            w_user = tf.get_variable(\"embd_user\", shape=[user_num, dim],\n",
    "                                     initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            w_item = tf.get_variable(\"embd_item\", shape=[item_num, dim],\n",
    "                                     initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "            embd_user = tf.nn.embedding_lookup(w_user, user_batch, name=\"embedding_user\")\n",
    "            embd_item = tf.nn.embedding_lookup(w_item, item_batch, name=\"embedding_item\")\n",
    "    \n",
    "    with tf.device(device):\n",
    "        infer = tf.reduce_sum(tf.multiply(embd_user, embd_item), 1)\n",
    "        infer = tf.add(infer, bias_global)\n",
    "        infer = tf.add(infer, bias_user)\n",
    "        infer = tf.add(infer, bias_item, name=\"svd_inference\")\n",
    "        regularizer = tf.add(tf.nn.l2_loss(embd_user), tf.nn.l2_loss(embd_item), \n",
    "                             name=\"svd_regularizer\")\n",
    "    return infer, regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(infer, regularizer, rate_batch, learning_rate=0.001, reg=0.1, device=\"/cpu:0\"):\n",
    "    with tf.device(device):\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
    "        penalty = tf.constant(reg, dtype=tf.float32, shape=[], name=\"l2\")\n",
    "        cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))\n",
    "        train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    return cost, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples 9234, test samples 1027, samples per batch 92\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = get_data(df)\n",
    "\n",
    "samples_per_batch = len(df_train) // batch_size\n",
    "print(\"Number of train samples %d, test samples %d, samples per batch %d\" % \n",
    "      (len(df_train), len(df_test), samples_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     AKAVVQMXSAIGX\n",
      "1     A6KYDNP84GGGJ\n",
      "2    A2MPM6M93OXIJT\n",
      "3     ASJAKT8DJIAC5\n",
      "4     AJH2W783HOXZV\n",
      "Name: reviewerID, dtype: object\n",
      "0    A3E6FHS8DV037H\n",
      "1    A1W3CEEQBJ4GTN\n",
      "2    A3TOND09136H4A\n",
      "3    A3W2VF6D09B2RN\n",
      "4    A1N82BBPB5816P\n",
      "Name: reviewerID, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"reviewerID\"].head()) \n",
    "print(df_test[\"reviewerID\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    B0002GWFEQ\n",
      "1    B000NGVQKO\n",
      "2    B0002D0CLM\n",
      "3    B0002DV7U2\n",
      "4    B0002GXZK4\n",
      "Name: asin, dtype: object\n",
      "0    B000KW2YEI\n",
      "1    B00AK7SKL4\n",
      "2    B001PGXHX0\n",
      "3    B00064TZYW\n",
      "4    B001FB5Z44\n",
      "Name: asin, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"asin\"].head())\n",
    "print(df_test[\"asin\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5\n",
      "1    4\n",
      "2    3\n",
      "3    5\n",
      "4    5\n",
      "Name: overall, dtype: int64\n",
      "0    4\n",
      "1    3\n",
      "2    5\n",
      "3    5\n",
      "4    5\n",
      "Name: overall, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"overall\"].head())\n",
    "print(df_test[\"overall\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleIterator(object):\n",
    "    \"\"\"\n",
    "    Randomly generate batches\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,))\n",
    "        out = self.inputs[ids, :]\n",
    "        return [out[:, i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneEpochIterator(ShuffleIterator):\n",
    "    \"\"\"\n",
    "    Sequentially generate one-epoch batches, typically for test data\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        super(OneEpochIterator, self).__init__(inputs, batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size))\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0705 00:05:22.203510 140619911264064 deprecation.py:506] From /home/eloise/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "iter_train = ShuffleIterator([df_train[\"reviewerID\"],\n",
    "                                     df_train[\"asin\"],\n",
    "                                     df_train[\"overall\"]],\n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "\n",
    "iter_test = OneEpochIterator([df_test[\"reviewerID\"],\n",
    "                                     df_test[\"asin\"],\n",
    "                                     df_test[\"overall\"]],\n",
    "                                     batch_size=-1)\n",
    "\n",
    "user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "infer, regularizer = model(user_batch, item_batch, user_num=u_num, item_num=i_num, dim=dims, device=place_device)\n",
    "_, train_op = loss(infer, regularizer, rate_batch, learning_rate=0.0010, reg=0.05, device=place_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tTrain Error\tVal Error\tElapsed Time\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'int'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'A3SS4Y0BDSPDB0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c0ea9b3dff42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch: users,\n\u001b[1;32m     14\u001b[0m                                                                \u001b[0mitem_batch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                                                rate_batch: rates})\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mpred_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_batch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'A3SS4Y0BDSPDB0'"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" % (\"Epoch\", \"Train Error\", \"Val Error\", \"Elapsed Time\"))\n",
    "    errors = deque(maxlen=samples_per_batch)\n",
    "    start = time.time()\n",
    "    for i in range(max_epochs * samples_per_batch):\n",
    "        users, items, rates = next(iter_train)\n",
    "        print(type(users[0]))\n",
    "        print(type(items[0]))\n",
    "        print(type(rates[0]))\n",
    "        _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch: users,\n",
    "                                                               item_batch: items,\n",
    "                                                               rate_batch: rates})\n",
    "        pred_batch = clip(pred_batch)\n",
    "        errors.append(np.power(pred_batch - rates, 2))\n",
    "        if i % samples_per_batch == 0:\n",
    "            train_err = np.sqrt(np.mean(errors))\n",
    "            test_err2 = np.array([])\n",
    "            for users, items, rates in iter_test:\n",
    "                pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                        item_batch: items})\n",
    "                pred_batch = clip(pred_batch)\n",
    "                test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "            end = time.time()\n",
    "            \n",
    "            print(\"%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\" % (i // samples_per_batch, train_err, np.sqrt(np.mean(test_err2)), end - start))\n",
    "            start = end\n",
    "\n",
    "    saver.save(sess, './save/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
